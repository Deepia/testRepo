<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Google.Cloud.VideoIntelligence.V1</name>
    </assembly>
    <members>
        <member name="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoException">
            <summary>
            An error occurring when annotating an video.
            </summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoException.Response">
            <summary>
            The complete response containing the error.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.AnnotateVideoException.#ctor(Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults)">
            <summary>
            Constructs an exception based on the error in <paramref name="response"/>.
            </summary>
            <param name="response">The response containing the error. Must not be null, and the <see cref="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Error"/>
            property must not be null.</param>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoResponse">
            <summary>
            Video annotation response. Included in the `response`
            field of the `Operation` returned by the `GetOperation`
            call of the `google::longrunning::Operations` service.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.AnnotateVideoResponse.ThrowOnAnyError">
            <summary>
            If the <see cref="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Error"/> property is non-null for any response within <see cref="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoResponse.AnnotationResults"/>,
            throws an <see cref="T:System.AggregateException"/>, containing one <see cref="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoException"/>
            for each failed response. Otherwise, returns <c>this</c> (so that the method can be called in a fluent manner).
            </summary>
            <exception cref="T:System.AggregateException">The <see cref="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Error"/> property is non-null on one or
            more element of <see cref="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoResponse.AnnotationResults"/>.</exception>
            <returns><c>this</c> if no responses contain errors.</returns>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoResponse.AnnotationResultsFieldNumber">
            <summary>Field number for the "annotation_results" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoResponse.AnnotationResults">
            <summary>
            Annotation results for all videos specified in `AnnotateVideoRequest`.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults">
            <summary>
            Annotation results for a single video.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ThrowOnError">
            <summary>
            If the <see cref="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Error"/> property is non-null, throws an <see cref="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoException"/>.
            Otherwise, returns <c>this</c> (so that the method can be called in a fluent manner).
            </summary>
            <exception cref="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoException">The <see cref="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Error"/> property is non-null.</exception>
            <returns><c>this</c> if the message has no error.</returns>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.InputUriFieldNumber">
            <summary>Field number for the "input_uri" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.InputUri">
            <summary>
            Video file location in
            [Google Cloud Storage](https://cloud.google.com/storage/).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SegmentFieldNumber">
            <summary>Field number for the "segment" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Segment">
            <summary>
            Video segment on which the annotation is run.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SegmentLabelAnnotationsFieldNumber">
            <summary>Field number for the "segment_label_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SegmentLabelAnnotations">
            <summary>
            Topical label annotations on video level or user specified segment level.
            There is exactly one element for each unique label.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SegmentPresenceLabelAnnotationsFieldNumber">
            <summary>Field number for the "segment_presence_label_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SegmentPresenceLabelAnnotations">
            <summary>
            Presence label annotations on video level or user specified segment level.
            There is exactly one element for each unique label. This will eventually
            get publicly exposed and the restriction will be removed.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ShotLabelAnnotationsFieldNumber">
            <summary>Field number for the "shot_label_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ShotLabelAnnotations">
            <summary>
            Topical label annotations on shot level.
            There is exactly one element for each unique label.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ShotPresenceLabelAnnotationsFieldNumber">
            <summary>Field number for the "shot_presence_label_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ShotPresenceLabelAnnotations">
            <summary>
            Presence label annotations on shot level. There is exactly one element for
            each unique label. This will eventually get publicly exposed and the
            restriction will be removed.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.FrameLabelAnnotationsFieldNumber">
            <summary>Field number for the "frame_label_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.FrameLabelAnnotations">
            <summary>
            Label annotations on frame level.
            There is exactly one element for each unique label.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.FaceAnnotationsFieldNumber">
            <summary>Field number for the "face_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.FaceAnnotations">
            <summary>
            Face annotations. There is exactly one element for each unique face.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ShotAnnotationsFieldNumber">
            <summary>Field number for the "shot_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ShotAnnotations">
            <summary>
            Shot annotations. Each shot is represented as a video segment.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ExplicitAnnotationFieldNumber">
            <summary>Field number for the "explicit_annotation" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ExplicitAnnotation">
            <summary>
            Explicit content annotation.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SpeechTranscriptionsFieldNumber">
            <summary>Field number for the "speech_transcriptions" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.SpeechTranscriptions">
            <summary>
            Speech transcription.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.TextAnnotationsFieldNumber">
            <summary>Field number for the "text_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.TextAnnotations">
            <summary>
            OCR text detection and tracking.
            Annotations for list of detected text snippets. Each will have list of
            frame information associated with it.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ObjectAnnotationsFieldNumber">
            <summary>Field number for the "object_annotations" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ObjectAnnotations">
            <summary>
            Annotations for list of objects detected and tracked in video.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.ErrorFieldNumber">
            <summary>Field number for the "error" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationResults.Error">
            <summary>
            If set, indicates an error. Note that for a single `AnnotateVideoRequest`
            some videos may succeed and some may fail.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceReflection">
            <summary>Holder for reflection information generated from google/cloud/videointelligence/v1/video_intelligence.proto</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceReflection.Descriptor">
            <summary>File descriptor for google/cloud/videointelligence/v1/video_intelligence.proto</summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.Feature">
            <summary>
            Video annotation feature.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.Unspecified">
            <summary>
            Unspecified.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.LabelDetection">
            <summary>
            Label detection. Detect objects, such as dog or flower.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.ShotChangeDetection">
            <summary>
            Shot change detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.ExplicitContentDetection">
            <summary>
            Explicit content detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.FaceDetection">
            <summary>
            Human face detection and tracking.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.SpeechTranscription">
            <summary>
            Speech transcription.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.TextDetection">
            <summary>
            OCR text detection and tracking.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Feature.ObjectTracking">
            <summary>
            Object detection and tracking.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.LabelDetectionMode">
            <summary>
            Label detection mode.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionMode.Unspecified">
            <summary>
            Unspecified.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionMode.ShotMode">
            <summary>
            Detect shot-level labels.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionMode.FrameMode">
            <summary>
            Detect frame-level labels.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionMode.ShotAndFrameMode">
            <summary>
            Detect both shot-level and frame-level labels.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.Likelihood">
            <summary>
            Bucketized representation of likelihood.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Likelihood.Unspecified">
            <summary>
            Unspecified likelihood.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Likelihood.VeryUnlikely">
            <summary>
            Very unlikely.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Likelihood.Unlikely">
            <summary>
            Unlikely.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Likelihood.Possible">
            <summary>
            Possible.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Likelihood.Likely">
            <summary>
            Likely.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Likelihood.VeryLikely">
            <summary>
            Very likely.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest">
            <summary>
            Video annotation request.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.InputUriFieldNumber">
            <summary>Field number for the "input_uri" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.InputUri">
            <summary>
            Input video location. Currently, only
            [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
            supported, which must be specified in the following format:
            `gs://bucket-id/object-id` (other URI formats return
            [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
            more information, see [Request URIs](/storage/docs/reference-uris). A video
            URI may include wildcards in `object-id`, and thus identify multiple
            videos. Supported wildcards: '*' to match 0 or more characters;
            '?' to match 1 character. If unset, the input video should be embedded
            in the request as `input_content`. If set, `input_content` should be unset.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.InputContentFieldNumber">
            <summary>Field number for the "input_content" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.InputContent">
            <summary>
            The video data bytes.
            If unset, the input video(s) should be specified via `input_uri`.
            If set, `input_uri` should be unset.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.FeaturesFieldNumber">
            <summary>Field number for the "features" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.Features">
            <summary>
            Requested video annotation features.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.VideoContextFieldNumber">
            <summary>Field number for the "video_context" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.VideoContext">
            <summary>
            Additional video context and/or feature-specific parameters.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.OutputUriFieldNumber">
            <summary>Field number for the "output_uri" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.OutputUri">
            <summary>
            Optional location where the output (in JSON format) should be stored.
            Currently, only [Google Cloud Storage](https://cloud.google.com/storage/)
            URIs are supported, which must be specified in the following format:
            `gs://bucket-id/object-id` (other URI formats return
            [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
            more information, see [Request URIs](/storage/docs/reference-uris).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.LocationIdFieldNumber">
            <summary>Field number for the "location_id" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest.LocationId">
            <summary>
            Optional cloud region where annotation should take place. Supported cloud
            regions: `us-east1`, `us-west1`, `europe-west1`, `asia-east1`. If no region
            is specified, a region will be determined based on video file location.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoContext">
            <summary>
            Video context and/or feature-specific parameters.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.SegmentsFieldNumber">
            <summary>Field number for the "segments" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.Segments">
            <summary>
            Video segments to annotate. The segments may overlap and are not required
            to be contiguous or span the whole video. If unspecified, each video is
            treated as a single segment.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.LabelDetectionConfigFieldNumber">
            <summary>Field number for the "label_detection_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.LabelDetectionConfig">
            <summary>
            Config for LABEL_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.ShotChangeDetectionConfigFieldNumber">
            <summary>Field number for the "shot_change_detection_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.ShotChangeDetectionConfig">
            <summary>
            Config for SHOT_CHANGE_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.ExplicitContentDetectionConfigFieldNumber">
            <summary>Field number for the "explicit_content_detection_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.ExplicitContentDetectionConfig">
            <summary>
            Config for EXPLICIT_CONTENT_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.FaceDetectionConfigFieldNumber">
            <summary>Field number for the "face_detection_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.FaceDetectionConfig">
            <summary>
            Config for FACE_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.SpeechTranscriptionConfigFieldNumber">
            <summary>Field number for the "speech_transcription_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.SpeechTranscriptionConfig">
            <summary>
            Config for SPEECH_TRANSCRIPTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.TextDetectionConfigFieldNumber">
            <summary>Field number for the "text_detection_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.TextDetectionConfig">
            <summary>
            Config for TEXT_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoContext.ObjectTrackingConfigFieldNumber">
            <summary>Field number for the "object_tracking_config" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoContext.ObjectTrackingConfig">
            <summary>
            Config for OBJECT_TRACKING.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig">
            <summary>
            Config for LABEL_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.LabelDetectionModeFieldNumber">
            <summary>Field number for the "label_detection_mode" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.LabelDetectionMode">
            <summary>
            What labels should be detected with LABEL_DETECTION, in addition to
            video-level labels or segment-level labels.
            If unspecified, defaults to `SHOT_MODE`.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.StationaryCameraFieldNumber">
            <summary>Field number for the "stationary_camera" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.StationaryCamera">
            <summary>
            Whether the video has been shot from a stationary (i.e. non-moving) camera.
            When set to true, might improve detection accuracy for moving objects.
            Should be used with `SHOT_AND_FRAME_MODE` enabled.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.ModelFieldNumber">
            <summary>Field number for the "model" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.Model">
            <summary>
            Model to use for label detection.
            Supported values: "builtin/stable" (the default if unset) and
            "builtin/latest".
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.FrameConfidenceThresholdFieldNumber">
            <summary>Field number for the "frame_confidence_threshold" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.FrameConfidenceThreshold">
            <summary>
            The confidence threshold we perform filtering on the labels from
            frame-level detection. If not set, it is set to 0.4 by default. The valid
            range for this threshold is [0.1, 0.9]. Any value set outside of this
            range will be clipped.
            Note: for best results please follow the default threshold. We will update
            the default threshold everytime when we release a new model.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.VideoConfidenceThresholdFieldNumber">
            <summary>Field number for the "video_confidence_threshold" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelDetectionConfig.VideoConfidenceThreshold">
            <summary>
            The confidence threshold we perform filtering on the labels from
            video-level and shot-level detections. If not set, it is set to 0.3 by
            default. The valid range for this threshold is [0.1, 0.9]. Any value set
            outside of this range will be clipped.
            Note: for best results please follow the default threshold. We will update
            the default threshold everytime when we release a new model.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ShotChangeDetectionConfig">
            <summary>
            Config for SHOT_CHANGE_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ShotChangeDetectionConfig.ModelFieldNumber">
            <summary>Field number for the "model" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ShotChangeDetectionConfig.Model">
            <summary>
            Model to use for shot change detection.
            Supported values: "builtin/stable" (the default if unset) and
            "builtin/latest".
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ExplicitContentDetectionConfig">
            <summary>
            Config for EXPLICIT_CONTENT_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ExplicitContentDetectionConfig.ModelFieldNumber">
            <summary>Field number for the "model" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ExplicitContentDetectionConfig.Model">
            <summary>
            Model to use for explicit content detection.
            Supported values: "builtin/stable" (the default if unset) and
            "builtin/latest".
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.FaceDetectionConfig">
            <summary>
            Config for FACE_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceDetectionConfig.ModelFieldNumber">
            <summary>Field number for the "model" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceDetectionConfig.Model">
            <summary>
            Model to use for face detection.
            Supported values: "builtin/stable" (the default if unset) and
            "builtin/latest".
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceDetectionConfig.IncludeBoundingBoxesFieldNumber">
            <summary>Field number for the "include_bounding_boxes" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceDetectionConfig.IncludeBoundingBoxes">
            <summary>
            Whether bounding boxes be included in the face annotation output.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ObjectTrackingConfig">
            <summary>
            Config for OBJECT_TRACKING.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingConfig.ModelFieldNumber">
            <summary>Field number for the "model" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingConfig.Model">
            <summary>
            Model to use for object tracking.
            Supported values: "builtin/stable" (the default if unset) and
            "builtin/latest".
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.TextDetectionConfig">
            <summary>
            Config for TEXT_DETECTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextDetectionConfig.LanguageHintsFieldNumber">
            <summary>Field number for the "language_hints" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextDetectionConfig.LanguageHints">
             <summary>
             Language hint can be specified if the language to be detected is known a
             priori. It can increase the accuracy of the detection. Language hint must
             be language code in BCP-47 format.
            
             Automatic language detection is performed if no hint is provided.
             </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextDetectionConfig.ModelFieldNumber">
            <summary>Field number for the "model" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextDetectionConfig.Model">
            <summary>
            Model to use for text detection.
            Supported values: "builtin/stable" (the default if unset) and
            "builtin/latest".
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoSegment">
            <summary>
            Video segment.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoSegment.StartTimeOffsetFieldNumber">
            <summary>Field number for the "start_time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoSegment.StartTimeOffset">
            <summary>
            Time-offset, relative to the beginning of the video,
            corresponding to the start of the segment (inclusive).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoSegment.EndTimeOffsetFieldNumber">
            <summary>Field number for the "end_time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoSegment.EndTimeOffset">
            <summary>
            Time-offset, relative to the beginning of the video,
            corresponding to the end of the segment (inclusive).
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.LabelSegment">
            <summary>
            Video segment level annotation results for label detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelSegment.SegmentFieldNumber">
            <summary>Field number for the "segment" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelSegment.Segment">
            <summary>
            Video segment where a label was detected.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelSegment.ConfidenceFieldNumber">
            <summary>Field number for the "confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelSegment.Confidence">
            <summary>
            Confidence that the label is accurate. Range: [0, 1].
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.LabelFrame">
            <summary>
            Video frame level annotation results for label detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelFrame.TimeOffsetFieldNumber">
            <summary>Field number for the "time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelFrame.TimeOffset">
            <summary>
            Time-offset, relative to the beginning of the video, corresponding to the
            video frame for this location.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelFrame.ConfidenceFieldNumber">
            <summary>Field number for the "confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelFrame.Confidence">
            <summary>
            Confidence that the label is accurate. Range: [0, 1].
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.Entity">
            <summary>
            Detected entity from video analysis.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Entity.EntityIdFieldNumber">
            <summary>Field number for the "entity_id" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.Entity.EntityId">
            <summary>
            Opaque entity ID. Some IDs may be available in
            [Google Knowledge Graph Search
            API](https://developers.google.com/knowledge-graph/).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Entity.DescriptionFieldNumber">
            <summary>Field number for the "description" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.Entity.Description">
            <summary>
            Textual description, e.g. `Fixed-gear bicycle`.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.Entity.LanguageCodeFieldNumber">
            <summary>Field number for the "language_code" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.Entity.LanguageCode">
            <summary>
            Language code for `description` in BCP-47 format.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.LabelAnnotation">
            <summary>
            Label annotation.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.EntityFieldNumber">
            <summary>Field number for the "entity" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.Entity">
            <summary>
            Detected entity.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.CategoryEntitiesFieldNumber">
            <summary>Field number for the "category_entities" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.CategoryEntities">
            <summary>
            Common categories for the detected entity.
            E.g. when the label is `Terrier` the category is likely `dog`. And in some
            cases there might be more than one categories e.g. `Terrier` could also be
            a `pet`.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.SegmentsFieldNumber">
            <summary>Field number for the "segments" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.Segments">
            <summary>
            All video segments where a label was detected.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.FramesFieldNumber">
            <summary>Field number for the "frames" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.LabelAnnotation.Frames">
            <summary>
            All video frames where a label was detected.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ExplicitContentFrame">
            <summary>
            Video frame level annotation results for explicit content.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ExplicitContentFrame.TimeOffsetFieldNumber">
            <summary>Field number for the "time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ExplicitContentFrame.TimeOffset">
            <summary>
            Time-offset, relative to the beginning of the video, corresponding to the
            video frame for this location.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ExplicitContentFrame.PornographyLikelihoodFieldNumber">
            <summary>Field number for the "pornography_likelihood" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ExplicitContentFrame.PornographyLikelihood">
            <summary>
            Likelihood of the pornography content..
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ExplicitContentAnnotation">
            <summary>
            Explicit content annotation (based on per-frame visual signals only).
            If no explicit content has been detected in a frame, no annotations are
            present for that frame.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ExplicitContentAnnotation.FramesFieldNumber">
            <summary>Field number for the "frames" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ExplicitContentAnnotation.Frames">
            <summary>
            All video frames where explicit content was detected.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox">
            <summary>
            Normalized bounding box.
            The normalized vertex coordinates are relative to the original image.
            Range: [0, 1].
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.LeftFieldNumber">
            <summary>Field number for the "left" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.Left">
            <summary>
            Left X coordinate.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.TopFieldNumber">
            <summary>Field number for the "top" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.Top">
            <summary>
            Top Y coordinate.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.RightFieldNumber">
            <summary>Field number for the "right" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.Right">
            <summary>
            Right X coordinate.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.BottomFieldNumber">
            <summary>Field number for the "bottom" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingBox.Bottom">
            <summary>
            Bottom Y coordinate.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.FaceSegment">
            <summary>
            Video segment level annotation results for face detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceSegment.SegmentFieldNumber">
            <summary>Field number for the "segment" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceSegment.Segment">
            <summary>
            Video segment where a face was detected.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.FaceFrame">
            <summary>
            Video frame level annotation results for face detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceFrame.NormalizedBoundingBoxesFieldNumber">
            <summary>Field number for the "normalized_bounding_boxes" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceFrame.NormalizedBoundingBoxes">
            <summary>
            Normalized Bounding boxes in a frame.
            There can be more than one boxes if the same face is detected in multiple
            locations within the current frame.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceFrame.TimeOffsetFieldNumber">
            <summary>Field number for the "time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceFrame.TimeOffset">
            <summary>
            Time-offset, relative to the beginning of the video,
            corresponding to the video frame for this location.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.FaceAnnotation">
            <summary>
            Face annotation.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceAnnotation.ThumbnailFieldNumber">
            <summary>Field number for the "thumbnail" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceAnnotation.Thumbnail">
            <summary>
            Thumbnail of a representative face view (in JPEG format).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceAnnotation.SegmentsFieldNumber">
            <summary>Field number for the "segments" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceAnnotation.Segments">
            <summary>
            All video segments where a face was detected.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.FaceAnnotation.FramesFieldNumber">
            <summary>Field number for the "frames" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.FaceAnnotation.Frames">
            <summary>
            All video frames where a face was detected.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress">
            <summary>
            Annotation progress for a single video.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.InputUriFieldNumber">
            <summary>Field number for the "input_uri" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.InputUri">
            <summary>
            Video file location in
            [Google Cloud Storage](https://cloud.google.com/storage/).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.ProgressPercentFieldNumber">
            <summary>Field number for the "progress_percent" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.ProgressPercent">
            <summary>
            Approximate percentage processed thus far. Guaranteed to be
            100 when fully processed.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.StartTimeFieldNumber">
            <summary>Field number for the "start_time" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.StartTime">
            <summary>
            Time when the request was received.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.UpdateTimeFieldNumber">
            <summary>Field number for the "update_time" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.UpdateTime">
            <summary>
            Time of the most recent update.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.FeatureFieldNumber">
            <summary>Field number for the "feature" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.Feature">
            <summary>
            Specifies which feature is being tracked if the request contains more than
            one features.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.SegmentFieldNumber">
            <summary>Field number for the "segment" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoAnnotationProgress.Segment">
            <summary>
            Specifies which segment is being tracked if the request contains more than
            one segments.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.AnnotateVideoProgress">
            <summary>
            Video annotation progress. Included in the `metadata`
            field of the `Operation` returned by the `GetOperation`
            call of the `google::longrunning::Operations` service.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.AnnotateVideoProgress.AnnotationProgressFieldNumber">
            <summary>Field number for the "annotation_progress" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.AnnotateVideoProgress.AnnotationProgress">
            <summary>
            Progress metadata for all videos specified in `AnnotateVideoRequest`.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig">
            <summary>
            Config for SPEECH_TRANSCRIPTION.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.LanguageCodeFieldNumber">
            <summary>Field number for the "language_code" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.LanguageCode">
            <summary>
            *Required* The language of the supplied audio as a
            [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag.
            Example: "en-US".
            See [Language Support](https://cloud.google.com/speech/docs/languages)
            for a list of the currently supported language codes.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.MaxAlternativesFieldNumber">
            <summary>Field number for the "max_alternatives" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.MaxAlternatives">
            <summary>
            *Optional* Maximum number of recognition hypotheses to be returned.
            Specifically, the maximum number of `SpeechRecognitionAlternative` messages
            within each `SpeechTranscription`. The server may return fewer than
            `max_alternatives`. Valid values are `0`-`30`. A value of `0` or `1` will
            return a maximum of one. If omitted, will return a maximum of one.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.FilterProfanityFieldNumber">
            <summary>Field number for the "filter_profanity" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.FilterProfanity">
            <summary>
            *Optional* If set to `true`, the server will attempt to filter out
            profanities, replacing all but the initial character in each filtered word
            with asterisks, e.g. "f***". If set to `false` or omitted, profanities
            won't be filtered out.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.SpeechContextsFieldNumber">
            <summary>Field number for the "speech_contexts" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.SpeechContexts">
            <summary>
            *Optional* A means to provide context to assist the speech recognition.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.EnableAutomaticPunctuationFieldNumber">
            <summary>Field number for the "enable_automatic_punctuation" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.EnableAutomaticPunctuation">
            <summary>
            *Optional* If 'true', adds punctuation to recognition result hypotheses.
            This feature is only available in select languages. Setting this for
            requests in other languages has no effect at all. The default 'false' value
            does not add punctuation to result hypotheses. NOTE: "This is currently
            offered as an experimental service, complimentary to all users. In the
            future this may be exclusively available as a premium feature."
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.AudioTracksFieldNumber">
            <summary>Field number for the "audio_tracks" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.AudioTracks">
            <summary>
            *Optional* For file formats, such as MXF or MKV, supporting multiple audio
            tracks, specify up to two tracks. Default: track 0.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.EnableSpeakerDiarizationFieldNumber">
            <summary>Field number for the "enable_speaker_diarization" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.EnableSpeakerDiarization">
            <summary>
            *Optional* If 'true', enables speaker detection for each recognized word in
            the top alternative of the recognition result using a speaker_tag provided
            in the WordInfo.
            Note: When this is true, we send all the words from the beginning of the
            audio for the top alternative in every consecutive responses.
            This is done in order to improve our speaker tags as our models learn to
            identify the speakers in the conversation over time.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.DiarizationSpeakerCountFieldNumber">
            <summary>Field number for the "diarization_speaker_count" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.DiarizationSpeakerCount">
            <summary>
            *Optional*
            If set, specifies the estimated number of speakers in the conversation.
            If not set, defaults to '2'.
            Ignored unless enable_speaker_diarization is set to true.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.EnableWordConfidenceFieldNumber">
            <summary>Field number for the "enable_word_confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscriptionConfig.EnableWordConfidence">
            <summary>
            *Optional* If `true`, the top result includes a list of words and the
            confidence for those words. If `false`, no word-level confidence
            information is returned. The default is `false`.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.SpeechContext">
            <summary>
            Provides "hints" to the speech recognizer to favor specific words and phrases
            in the results.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechContext.PhrasesFieldNumber">
            <summary>Field number for the "phrases" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechContext.Phrases">
            <summary>
            *Optional* A list of strings containing words and phrases "hints" so that
            the speech recognition is more likely to recognize them. This can be used
            to improve the accuracy for specific words and phrases, for example, if
            specific commands are typically spoken by the user. This can also be used
            to add additional words to the vocabulary of the recognizer. See
            [usage limits](https://cloud.google.com/speech/limits#content).
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.SpeechTranscription">
            <summary>
            A speech recognition result corresponding to a portion of the audio.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscription.AlternativesFieldNumber">
            <summary>Field number for the "alternatives" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscription.Alternatives">
            <summary>
            May contain one or more recognition hypotheses (up to the maximum specified
            in `max_alternatives`).  These alternatives are ordered in terms of
            accuracy, with the top (first) alternative being the most probable, as
            ranked by the recognizer.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechTranscription.LanguageCodeFieldNumber">
            <summary>Field number for the "language_code" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechTranscription.LanguageCode">
            <summary>
            Output only. The
            [BCP-47](https://www.rfc-editor.org/rfc/bcp/bcp47.txt) language tag of the
            language in this result. This language code was detected to have the most
            likelihood of being spoken in the audio.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative">
            <summary>
            Alternative hypotheses (a.k.a. n-best list).
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative.TranscriptFieldNumber">
            <summary>Field number for the "transcript" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative.Transcript">
            <summary>
            Transcript text representing the words that the user spoke.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative.ConfidenceFieldNumber">
            <summary>Field number for the "confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative.Confidence">
            <summary>
            Output only. The confidence estimate between 0.0 and 1.0. A higher number
            indicates an estimated greater likelihood that the recognized words are
            correct. This field is set only for the top alternative.
            This field is not guaranteed to be accurate and users should not rely on it
            to be always provided.
            The default of 0.0 is a sentinel value indicating `confidence` was not set.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative.WordsFieldNumber">
            <summary>Field number for the "words" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.SpeechRecognitionAlternative.Words">
            <summary>
            Output only. A list of word-specific information for each recognized word.
            Note: When `enable_speaker_diarization` is true, you will see all the words
            from the beginning of the audio.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.WordInfo">
            <summary>
            Word-specific information for recognized words. Word information is only
            included in the response when certain request parameters are set, such
            as `enable_word_time_offsets`.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.WordInfo.StartTimeFieldNumber">
            <summary>Field number for the "start_time" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.WordInfo.StartTime">
            <summary>
            Time offset relative to the beginning of the audio, and
            corresponding to the start of the spoken word. This field is only set if
            `enable_word_time_offsets=true` and only in the top hypothesis. This is an
            experimental feature and the accuracy of the time offset can vary.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.WordInfo.EndTimeFieldNumber">
            <summary>Field number for the "end_time" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.WordInfo.EndTime">
            <summary>
            Time offset relative to the beginning of the audio, and
            corresponding to the end of the spoken word. This field is only set if
            `enable_word_time_offsets=true` and only in the top hypothesis. This is an
            experimental feature and the accuracy of the time offset can vary.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.WordInfo.WordFieldNumber">
            <summary>Field number for the "word" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.WordInfo.Word">
            <summary>
            The word corresponding to this set of information.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.WordInfo.ConfidenceFieldNumber">
            <summary>Field number for the "confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.WordInfo.Confidence">
            <summary>
            Output only. The confidence estimate between 0.0 and 1.0. A higher number
            indicates an estimated greater likelihood that the recognized words are
            correct. This field is set only for the top alternative.
            This field is not guaranteed to be accurate and users should not rely on it
            to be always provided.
            The default of 0.0 is a sentinel value indicating `confidence` was not set.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.WordInfo.SpeakerTagFieldNumber">
            <summary>Field number for the "speaker_tag" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.WordInfo.SpeakerTag">
            <summary>
            Output only. A distinct integer value is assigned for every speaker within
            the audio. This field specifies which one of those speakers was detected to
            have spoken this word. Value ranges from 1 up to diarization_speaker_count,
            and is only set if speaker diarization is enabled.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.NormalizedVertex">
            <summary>
            A vertex represents a 2D point in the image.
            NOTE: the normalized vertex coordinates are relative to the original image
            and range from 0 to 1.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedVertex.XFieldNumber">
            <summary>Field number for the "x" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedVertex.X">
            <summary>
            X coordinate.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedVertex.YFieldNumber">
            <summary>Field number for the "y" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedVertex.Y">
            <summary>
            Y coordinate.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingPoly">
             <summary>
             Normalized bounding polygon for text (that might not be aligned with axis).
             Contains list of the corner points in clockwise order starting from
             top-left corner. For example, for a rectangular bounding box:
             When the text is horizontal it might look like:
                     0----1
                     |    |
                     3----2
            
             When it's clockwise rotated 180 degrees around the top-left corner it
             becomes:
                     2----3
                     |    |
                     1----0
            
             and the vertex order will still be (0, 1, 2, 3). Note that values can be less
             than 0, or greater than 1 due to trignometric calculations for location of
             the box.
             </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingPoly.VerticesFieldNumber">
            <summary>Field number for the "vertices" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.NormalizedBoundingPoly.Vertices">
            <summary>
            Normalized vertices of the bounding polygon.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.TextSegment">
            <summary>
            Video segment level annotation results for text detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextSegment.SegmentFieldNumber">
            <summary>Field number for the "segment" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextSegment.Segment">
            <summary>
            Video segment where a text snippet was detected.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextSegment.ConfidenceFieldNumber">
            <summary>Field number for the "confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextSegment.Confidence">
            <summary>
            Confidence for the track of detected text. It is calculated as the highest
            over all frames where OCR detected text appears.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextSegment.FramesFieldNumber">
            <summary>Field number for the "frames" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextSegment.Frames">
            <summary>
            Information related to the frames where OCR detected text appears.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.TextFrame">
            <summary>
            Video frame level annotation results for text annotation (OCR).
            Contains information regarding timestamp and bounding box locations for the
            frames containing detected OCR text snippets.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextFrame.RotatedBoundingBoxFieldNumber">
            <summary>Field number for the "rotated_bounding_box" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextFrame.RotatedBoundingBox">
            <summary>
            Bounding polygon of the detected text for this frame.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextFrame.TimeOffsetFieldNumber">
            <summary>Field number for the "time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextFrame.TimeOffset">
            <summary>
            Timestamp of this frame.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.TextAnnotation">
            <summary>
            Annotations related to one detected OCR text snippet. This will contain the
            corresponding text, confidence value, and frame level information for each
            detection.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextAnnotation.TextFieldNumber">
            <summary>Field number for the "text" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextAnnotation.Text">
            <summary>
            The detected text.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.TextAnnotation.SegmentsFieldNumber">
            <summary>Field number for the "segments" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.TextAnnotation.Segments">
            <summary>
            All video segments where OCR detected text appears.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ObjectTrackingFrame">
            <summary>
            Video frame level annotations for object detection and tracking. This field
            stores per frame location, time offset, and confidence.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingFrame.NormalizedBoundingBoxFieldNumber">
            <summary>Field number for the "normalized_bounding_box" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingFrame.NormalizedBoundingBox">
            <summary>
            The normalized bounding box location of this object track for the frame.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingFrame.TimeOffsetFieldNumber">
            <summary>Field number for the "time_offset" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingFrame.TimeOffset">
            <summary>
            The timestamp of the frame in microseconds.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation">
            <summary>
            Annotations corresponding to one tracked object.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.SegmentFieldNumber">
            <summary>Field number for the "segment" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.Segment">
            <summary>
            Non-streaming batch mode ONLY.
            Each object track corresponds to one video segment where it appears.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.TrackIdFieldNumber">
            <summary>Field number for the "track_id" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.TrackId">
            <summary>
            Streaming mode ONLY.
            In streaming mode, we do not know the end time of a tracked object
            before it is completed. Hence, there is no VideoSegment info returned.
            Instead, we provide a unique identifiable integer track_id so that
            the customers can correlate the results of the ongoing
            ObjectTrackAnnotation of the same track_id over time.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.EntityFieldNumber">
            <summary>Field number for the "entity" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.Entity">
            <summary>
            Entity to specify the object category that this track is labeled as.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.ConfidenceFieldNumber">
            <summary>Field number for the "confidence" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.Confidence">
            <summary>
            Object category's labeling confidence of this track.
            </summary>
        </member>
        <member name="F:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.FramesFieldNumber">
            <summary>Field number for the "frames" field.</summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.Frames">
            <summary>
            Information corresponding to all frames where this object track appears.
            Non-streaming batch mode: it may be one or multiple ObjectTrackingFrame
            messages in frames.
            Streaming mode: it can only be one ObjectTrackingFrame message in frames.
            </summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.ObjectTrackingAnnotation.TrackInfoOneofCase">
            <summary>Enum of possible cases for the "track_info" oneof.</summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService">
            <summary>
            Service that implements Google Cloud Video Intelligence API.
            </summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.Descriptor">
            <summary>Service descriptor</summary>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceBase">
            <summary>Base class for server-side implementations of VideoIntelligenceService</summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceBase.AnnotateVideo(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Grpc.Core.ServerCallContext)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">The request received from the client.</param>
            <param name="context">The context of the server-side call handler being invoked.</param>
            <returns>The response to send back to the client (wrapped by a task).</returns>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient">
            <summary>Client for VideoIntelligenceService</summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.#ctor(Grpc.Core.Channel)">
            <summary>Creates a new client for VideoIntelligenceService</summary>
            <param name="channel">The channel to use to make remote calls.</param>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.#ctor(Grpc.Core.CallInvoker)">
            <summary>Creates a new client for VideoIntelligenceService that uses a custom <c>CallInvoker</c>.</summary>
            <param name="callInvoker">The callInvoker to use to make remote calls.</param>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.#ctor">
            <summary>Protected parameterless constructor to allow creation of test doubles.</summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.#ctor(Grpc.Core.ClientBase.ClientBaseConfiguration)">
            <summary>Protected constructor to allow creation of configured clients.</summary>
            <param name="configuration">The client configuration.</param>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.AnnotateVideo(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Grpc.Core.Metadata,System.Nullable{System.DateTime},System.Threading.CancellationToken)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">The request to send to the server.</param>
            <param name="headers">The initial metadata to send with the call. This parameter is optional.</param>
            <param name="deadline">An optional deadline for the call. The call will be cancelled if deadline is hit.</param>
            <param name="cancellationToken">An optional token for canceling the call.</param>
            <returns>The response received from the server.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.AnnotateVideo(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Grpc.Core.CallOptions)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">The request to send to the server.</param>
            <param name="options">The options for the call.</param>
            <returns>The response received from the server.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.AnnotateVideoAsync(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Grpc.Core.Metadata,System.Nullable{System.DateTime},System.Threading.CancellationToken)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">The request to send to the server.</param>
            <param name="headers">The initial metadata to send with the call. This parameter is optional.</param>
            <param name="deadline">An optional deadline for the call. The call will be cancelled if deadline is hit.</param>
            <param name="cancellationToken">An optional token for canceling the call.</param>
            <returns>The call object.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.AnnotateVideoAsync(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Grpc.Core.CallOptions)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">The request to send to the server.</param>
            <param name="options">The options for the call.</param>
            <returns>The call object.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.NewInstance(Grpc.Core.ClientBase.ClientBaseConfiguration)">
            <summary>Creates a new instance of client from given <c>ClientBaseConfiguration</c>.</summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient.CreateOperationsClient">
            <summary>
            Creates a new instance of <see cref="T:Google.LongRunning.Operations.OperationsClient"/> using the same call invoker as this client.
            </summary>
            <returns>A new Operations client for the same target as this client.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.BindService(Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceBase)">
            <summary>Creates service definition that can be registered with a server</summary>
            <param name="serviceImpl">An object implementing the server-side handling logic.</param>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.BindService(Grpc.Core.ServiceBinderBase,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceBase)">
            <summary>Register service method with a service binder with or without implementation. Useful when customizing the  service binding logic.
            Note: this method is part of an experimental API that can change or be removed without any prior notice.</summary>
            <param name="serviceBinder">Service methods will be bound by calling <c>AddMethod</c> on this object.</param>
            <param name="serviceImpl">An object implementing the server-side handling logic.</param>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings">
            <summary>
            Settings for a <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.GetDefault">
            <summary>
            Get a new instance of the default <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/>.
            </summary>
            <returns>
            A new instance of the default <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/>.
            </returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.#ctor">
            <summary>
            Constructs a new <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/> object with default settings.
            </summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.IdempotentRetryFilter">
            <summary>
            The filter specifying which RPC <see cref="T:Grpc.Core.StatusCode"/>s are eligible for retry
            for "Idempotent" <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods.
            </summary>
            <remarks>
            The eligible RPC <see cref="T:Grpc.Core.StatusCode"/>s for retry for "Idempotent" RPC methods are:
            <list type="bullet">
            <item><description><see cref="F:Grpc.Core.StatusCode.DeadlineExceeded"/></description></item>
            <item><description><see cref="F:Grpc.Core.StatusCode.Unavailable"/></description></item>
            </list>
            </remarks>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.NonIdempotentRetryFilter">
            <summary>
            The filter specifying which RPC <see cref="T:Grpc.Core.StatusCode"/>s are eligible for retry
            for "NonIdempotent" <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods.
            </summary>
            <remarks>
            There are no RPC <see cref="T:Grpc.Core.StatusCode"/>s eligible for retry for "NonIdempotent" RPC methods.
            </remarks>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.GetDefaultRetryBackoff">
            <summary>
            "Default" retry backoff for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods.
            </summary>
            <returns>
            The "Default" retry backoff for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods.
            </returns>
            <remarks>
            The "Default" retry backoff for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods is defined as:
            <list type="bullet">
            <item><description>Initial delay: 1000 milliseconds</description></item>
            <item><description>Maximum delay: 120000 milliseconds</description></item>
            <item><description>Delay multiplier: 2.5</description></item>
            </list>
            </remarks>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.GetDefaultTimeoutBackoff">
            <summary>
            "Default" timeout backoff for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods.
            </summary>
            <returns>
            The "Default" timeout backoff for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods.
            </returns>
            <remarks>
            The "Default" timeout backoff for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> RPC methods is defined as:
            <list type="bullet">
            <item><description>Initial timeout: 120000 milliseconds</description></item>
            <item><description>Timeout multiplier: 1.0</description></item>
            <item><description>Maximum timeout: 120000 milliseconds</description></item>
            </list>
            </remarks>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.AnnotateVideoSettings">
            <summary>
            <see cref="T:Google.Api.Gax.Grpc.CallSettings"/> for synchronous and asynchronous calls to
            <c>VideoIntelligenceServiceClient.AnnotateVideo</c> and <c>VideoIntelligenceServiceClient.AnnotateVideoAsync</c>.
            </summary>
            <remarks>
            The default <c>VideoIntelligenceServiceClient.AnnotateVideo</c> and
            <c>VideoIntelligenceServiceClient.AnnotateVideoAsync</c> <see cref="T:Google.Api.Gax.Grpc.RetrySettings"/> are:
            <list type="bullet">
            <item><description>Initial retry delay: 1000 milliseconds</description></item>
            <item><description>Retry delay multiplier: 2.5</description></item>
            <item><description>Retry maximum delay: 120000 milliseconds</description></item>
            <item><description>Initial timeout: 120000 milliseconds</description></item>
            <item><description>Timeout multiplier: 1.0</description></item>
            <item><description>Timeout maximum delay: 120000 milliseconds</description></item>
            </list>
            Retry will be attempted on the following response status codes:
            <list>
            <item><description><see cref="F:Grpc.Core.StatusCode.DeadlineExceeded"/></description></item>
            <item><description><see cref="F:Grpc.Core.StatusCode.Unavailable"/></description></item>
            </list>
            Default RPC expiration is 600000 milliseconds.
            </remarks>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.AnnotateVideoOperationsSettings">
            <summary>
            Long Running Operation settings for calls to <c>VideoIntelligenceServiceClient.AnnotateVideo</c>.
            </summary>
            <remarks>
            Uses default <see cref="T:Google.Api.Gax.PollSettings"/> of:
            <list type="bullet">
            <item><description>Initial delay: 20000 milliseconds</description></item>
            <item><description>Delay multiplier: 1.5</description></item>
            <item><description>Maximum delay: 45000 milliseconds</description></item>
            <item><description>Total timeout: 86400000 milliseconds</description></item>
            </list>
            </remarks>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings.Clone">
            <summary>
            Creates a deep clone of this object, with all the same property values.
            </summary>
            <returns>A deep clone of this <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/> object.</returns>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder">
            <summary>
            Builder class for <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> to provide simple configuration of credentials, endpoint etc.
            </summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder.Settings">
            <summary>
            The settings to use for RPCs, or null for the default settings.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder.Build">
            <inheritdoc/>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder.BuildAsync(System.Threading.CancellationToken)">
            <inheritdoc />
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder.GetDefaultEndpoint">
            <inheritdoc />
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder.GetDefaultScopes">
            <inheritdoc />
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientBuilder.GetChannelPool">
            <inheritdoc />
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient">
            <summary>
            VideoIntelligenceService client wrapper, for convenient use.
            </summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.DefaultEndpoint">
            <summary>
            The default endpoint for the VideoIntelligenceService service, which is a host of "videointelligence.googleapis.com" and a port of 443.
            </summary>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.DefaultScopes">
            <summary>
            The default VideoIntelligenceService scopes.
            </summary>
            <remarks>
            The default VideoIntelligenceService scopes are:
            <list type="bullet">
            <item><description>"https://www.googleapis.com/auth/cloud-platform"</description></item>
            </list>
            </remarks>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.CreateAsync(Google.Api.Gax.Grpc.ServiceEndpoint,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)">
            <summary>
            Asynchronously creates a <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>, applying defaults for all unspecified settings,
            and creating a channel connecting to the given endpoint with application default credentials where
            necessary. See the example for how to use custom credentials.
            </summary>
            <example>
            This sample shows how to create a client using default credentials:
            <code>
            using Google.Cloud.VideoIntelligence.V1;
            ...
            // When running on Google Cloud Platform this will use the project Compute Credential.
            // Or set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of a JSON
            // credential file to use that credential.
            VideoIntelligenceServiceClient client = await VideoIntelligenceServiceClient.CreateAsync();
            </code>
            This sample shows how to create a client using credentials loaded from a JSON file:
            <code>
            using Google.Cloud.VideoIntelligence.V1;
            using Google.Apis.Auth.OAuth2;
            using Grpc.Auth;
            using Grpc.Core;
            ...
            GoogleCredential cred = GoogleCredential.FromFile("/path/to/credentials.json");
            Channel channel = new Channel(
                VideoIntelligenceServiceClient.DefaultEndpoint.Host, VideoIntelligenceServiceClient.DefaultEndpoint.Port, cred.ToChannelCredentials());
            VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.Create(channel);
            ...
            // Shutdown the channel when it is no longer required.
            await channel.ShutdownAsync();
            </code>
            </example>
            <param name="endpoint">Optional <see cref="T:Google.Api.Gax.Grpc.ServiceEndpoint"/>.</param>
            <param name="settings">Optional <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/>.</param>
            <returns>The task representing the created <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.Create(Google.Api.Gax.Grpc.ServiceEndpoint,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)">
            <summary>
            Synchronously creates a <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>, applying defaults for all unspecified settings,
            and creating a channel connecting to the given endpoint with application default credentials where
            necessary. See the example for how to use custom credentials.
            </summary>
            <example>
            This sample shows how to create a client using default credentials:
            <code>
            using Google.Cloud.VideoIntelligence.V1;
            ...
            // When running on Google Cloud Platform this will use the project Compute Credential.
            // Or set the GOOGLE_APPLICATION_CREDENTIALS environment variable to the path of a JSON
            // credential file to use that credential.
            VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.Create();
            </code>
            This sample shows how to create a client using credentials loaded from a JSON file:
            <code>
            using Google.Cloud.VideoIntelligence.V1;
            using Google.Apis.Auth.OAuth2;
            using Grpc.Auth;
            using Grpc.Core;
            ...
            GoogleCredential cred = GoogleCredential.FromFile("/path/to/credentials.json");
            Channel channel = new Channel(
                VideoIntelligenceServiceClient.DefaultEndpoint.Host, VideoIntelligenceServiceClient.DefaultEndpoint.Port, cred.ToChannelCredentials());
            VideoIntelligenceServiceClient client = VideoIntelligenceServiceClient.Create(channel);
            ...
            // Shutdown the channel when it is no longer required.
            channel.ShutdownAsync().Wait();
            </code>
            </example>
            <param name="endpoint">Optional <see cref="T:Google.Api.Gax.Grpc.ServiceEndpoint"/>.</param>
            <param name="settings">Optional <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/>.</param>
            <returns>The created <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.Create(Grpc.Core.Channel,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)">
            <summary>
            Creates a <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> which uses the specified channel for remote operations.
            </summary>
            <param name="channel">The <see cref="T:Grpc.Core.Channel"/> for remote operations. Must not be null.</param>
            <param name="settings">Optional <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/>.</param>
            <returns>The created <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.Create(Grpc.Core.CallInvoker,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)">
            <summary>
            Creates a <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/> which uses the specified call invoker for remote operations.
            </summary>
            <param name="callInvoker">The <see cref="T:Grpc.Core.CallInvoker"/> for remote operations. Must not be null.</param>
            <param name="settings">Optional <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/>.</param>
            <returns>The created <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient"/>.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.ShutdownDefaultChannelsAsync">
            <summary>
            Shuts down any channels automatically created by <see cref="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.Create(Google.Api.Gax.Grpc.ServiceEndpoint,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)"/>
            and <see cref="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.CreateAsync(Google.Api.Gax.Grpc.ServiceEndpoint,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)"/>. Channels which weren't automatically
            created are not affected.
            </summary>
            <remarks>After calling this method, further calls to <see cref="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.Create(Google.Api.Gax.Grpc.ServiceEndpoint,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)"/>
            and <see cref="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.CreateAsync(Google.Api.Gax.Grpc.ServiceEndpoint,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)"/> will create new channels, which could
            in turn be shut down by another call to this method.</remarks>
            <returns>A task representing the asynchronous shutdown operation.</returns>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.GrpcClient">
            <summary>
            The underlying gRPC VideoIntelligenceService client.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.AnnotateVideoAsync(System.String,System.Collections.Generic.IEnumerable{Google.Cloud.VideoIntelligence.V1.Feature},Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="inputUri">
            Input video location. Currently, only
            [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
            supported, which must be specified in the following format:
            `gs://bucket-id/object-id` (other URI formats return
            [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
            more information, see [Request URIs](/storage/docs/reference-uris). A video
            URI may include wildcards in `object-id`, and thus identify multiple
            videos. Supported wildcards: '*' to match 0 or more characters;
            '?' to match 1 character. If unset, the input video should be embedded
            in the request as `input_content`. If set, `input_content` should be unset.
            </param>
            <param name="features">
            Requested video annotation features.
            </param>
            <param name="callSettings">
            If not null, applies overrides to this RPC call.
            </param>
            <returns>
            A Task containing the RPC response.
            </returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.AnnotateVideoAsync(System.String,System.Collections.Generic.IEnumerable{Google.Cloud.VideoIntelligence.V1.Feature},System.Threading.CancellationToken)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="inputUri">
            Input video location. Currently, only
            [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
            supported, which must be specified in the following format:
            `gs://bucket-id/object-id` (other URI formats return
            [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
            more information, see [Request URIs](/storage/docs/reference-uris). A video
            URI may include wildcards in `object-id`, and thus identify multiple
            videos. Supported wildcards: '*' to match 0 or more characters;
            '?' to match 1 character. If unset, the input video should be embedded
            in the request as `input_content`. If set, `input_content` should be unset.
            </param>
            <param name="features">
            Requested video annotation features.
            </param>
            <param name="cancellationToken">
            A <see cref="T:System.Threading.CancellationToken"/> to use for this RPC.
            </param>
            <returns>
            A Task containing the RPC response.
            </returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.AnnotateVideo(System.String,System.Collections.Generic.IEnumerable{Google.Cloud.VideoIntelligence.V1.Feature},Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="inputUri">
            Input video location. Currently, only
            [Google Cloud Storage](https://cloud.google.com/storage/) URIs are
            supported, which must be specified in the following format:
            `gs://bucket-id/object-id` (other URI formats return
            [google.rpc.Code.INVALID_ARGUMENT][google.rpc.Code.INVALID_ARGUMENT]). For
            more information, see [Request URIs](/storage/docs/reference-uris). A video
            URI may include wildcards in `object-id`, and thus identify multiple
            videos. Supported wildcards: '*' to match 0 or more characters;
            '?' to match 1 character. If unset, the input video should be embedded
            in the request as `input_content`. If set, `input_content` should be unset.
            </param>
            <param name="features">
            Requested video annotation features.
            </param>
            <param name="callSettings">
            If not null, applies overrides to this RPC call.
            </param>
            <returns>
            The RPC response.
            </returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.AnnotateVideoAsync(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">
            The request object containing all of the parameters for the API call.
            </param>
            <param name="callSettings">
            If not null, applies overrides to this RPC call.
            </param>
            <returns>
            A Task containing the RPC response.
            </returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.PollOnceAnnotateVideoAsync(System.String,Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Asynchronously poll an operation once, using an <c>operationName</c> from a previous invocation of <c>AnnotateVideoAsync</c>.
            </summary>
            <param name="operationName">The name of a previously invoked operation. Must not be <c>null</c> or empty.</param>
            <param name="callSettings">If not null, applies overrides to this RPC call.</param>
            <returns>A task representing the result of polling the operation.</returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.AnnotateVideo(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">
            The request object containing all of the parameters for the API call.
            </param>
            <param name="callSettings">
            If not null, applies overrides to this RPC call.
            </param>
            <returns>
            The RPC response.
            </returns>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.AnnotateVideoOperationsClient">
            <summary>
            The long-running operations client for <c>AnnotateVideo</c>.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClient.PollOnceAnnotateVideo(System.String,Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Poll an operation once, using an <c>operationName</c> from a previous invocation of <c>AnnotateVideo</c>.
            </summary>
            <param name="operationName">The name of a previously invoked operation. Must not be <c>null</c> or empty.</param>
            <param name="callSettings">If not null, applies overrides to this RPC call.</param>
            <returns>The result of polling the operation.</returns>
        </member>
        <member name="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientImpl">
            <summary>
            VideoIntelligenceService client wrapper implementation, for convenient use.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientImpl.#ctor(Google.Cloud.VideoIntelligence.V1.VideoIntelligenceService.VideoIntelligenceServiceClient,Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings)">
            <summary>
            Constructs a client wrapper for the VideoIntelligenceService service, with the specified gRPC client and settings.
            </summary>
            <param name="grpcClient">The underlying gRPC client.</param>
            <param name="settings">The base <see cref="T:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceSettings"/> used within this client </param>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientImpl.GrpcClient">
            <summary>
            The underlying gRPC VideoIntelligenceService client.
            </summary>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientImpl.AnnotateVideoAsync(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">
            The request object containing all of the parameters for the API call.
            </param>
            <param name="callSettings">
            If not null, applies overrides to this RPC call.
            </param>
            <returns>
            A Task containing the RPC response.
            </returns>
        </member>
        <member name="M:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientImpl.AnnotateVideo(Google.Cloud.VideoIntelligence.V1.AnnotateVideoRequest,Google.Api.Gax.Grpc.CallSettings)">
            <summary>
            Performs asynchronous video annotation. Progress and results can be
            retrieved through the `google.longrunning.Operations` interface.
            `Operation.metadata` contains `AnnotateVideoProgress` (progress).
            `Operation.response` contains `AnnotateVideoResponse` (results).
            </summary>
            <param name="request">
            The request object containing all of the parameters for the API call.
            </param>
            <param name="callSettings">
            If not null, applies overrides to this RPC call.
            </param>
            <returns>
            The RPC response.
            </returns>
        </member>
        <member name="P:Google.Cloud.VideoIntelligence.V1.VideoIntelligenceServiceClientImpl.AnnotateVideoOperationsClient">
            <summary>
            The long-running operations client for <c>AnnotateVideo</c>.
            </summary>
        </member>
    </members>
</doc>
